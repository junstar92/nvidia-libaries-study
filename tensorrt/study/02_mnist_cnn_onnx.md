# Table of Contents

- [Table of Contents](#table-of-contents)
- [MNIST Classification using ONNX Parser APIs](#mnist-classification-using-onnx-parser-apis)
- [Importing a Model Using the ONNX Parser](#importing-a-model-using-the-onnx-parser)
- [Performing Inference](#performing-inference)
- [References](#references)

<br>

# MNIST Classification using ONNX Parser APIs

이번 포스팅에서는 ONNX 파일을 파싱하여 네트워크 모델을 최적화하고 추론하는 방법에 대해서 살펴본다. 전체적인 내용은 [MNIST Classification using Network Definition APIs](/tensorrt/study/01_mnist_cnn_api.md)와 유사하며, 차이점은 네트워크를 Network Definition API가 아닌 ONNX Parser를 통해 ONNX 파일을 파싱하여 얻는 것이다.

ONNX 파일을 추출하는 방법은 아래의 주피터 노트북 파일의 마지막 셀을 참조하면 된다. 추출한 파일은 github에 첨부해두었고, 아래 두 번째 링크로 받을 수 있다.

- [Jupyter Notebook: Training simple mnist CNN model](/cudnn/code/mnist_cnn_v7/mnist-cnn.ipynb)
- [mnist_cnn.onnx](/tensorrt/code/mnist_cnn_onnx/mnist_cnn.onnx)

이렇게 ONNX로 추출한 네트워크를 Netron을 사용하여 시각화하면 아래와 같은 네트워크 구성을 확인할 수 있다.

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F5JSXs%2FbtsjlXqmKcW%2FlwYO8QtZgczM31xBwY5u6K%2Fimg.png" height=600px style="display: block; margin: 0 auto; background-color:white"/>

전체 코드와 사용한 샘플 데이터는 아래 링크에서 확인 가능하다.

- [mnist_cnn_onnx.cpp](/tensorrt/code/mnist_cnn_onnx/mnist_cnn_onnx.cpp)
- [Sample Mnist Data](/cudnn/code/mnist_cnn_v7/digits/) (0 ~ 9)

전체적인 코드 진행은 [MNIST Classification using Network Definition APIs](/tensorrt/study/01_mnist_cnn_api.md)와 거의 동일하며, 코드 또한 [mnist_cnn_api.cpp](/tensorrt/code/mnist_cnn_api/mnist_cnn_api.cpp)와 거의 유사하기 때문에 동일한 부분에 대한 내용은 이 포스팅에서는 생략한다. 전체적인 내용은 아래 링크를 참조하길 바란다.

- [MNIST Classification using Network Definition APIs](/tensorrt/study/01_mnist_cnn_api.md)

<br>

# Importing a Model Using the ONNX Parser

ONNX Parser를 통해 네트워크를 임포트하기 전까지의 과정은 Network Definition API를 사용하는 것과 동일하다. `IBuilder` 인스턴스를 생성하고, 이 인스턴스를 통해 먼저 `INetworkDefinition` 인스턴스를 생성한다.

그런 다음, `IParser` 인스턴스를 생성한다. 이 클래스는 `NvOnnxParser.h` 파일에 정의되어 있으며, 이 헤더의 namespace는 `nvonnxparser`이다. 이렇게 `IParser` 인스턴스를 생성한 뒤, 생성된 인스턴스의 `parseFromFile()` 메소드로 미리 준비해둔 ONNX 파일을 읽어서 네트워크를 파싱한다.
```c++
// create parser
IParser* parser = createParser(*network, gLogger);

// parsing onnx file
parser->parseFromFile("mnist_cnn.onnx", static_cast<int32_t>(ILogger::Severity::kVERBOSE));
```

그리고 이후 과정은 Network Definition API를 사용할 때와 모두 동일하다. 전후 과정에 대해 자세히 살펴보고 싶다면, 아래 링크를 참조바란다.

- [MNIST Classification using Network Definition APIs](/tensorrt/study/01_mnist_cnn_api.md)

로거의 출력 레벨을 `kINFO`로 설정하면, ONNX 파일을 파싱할 때 아래와 같은 출력을 확인할 수 있다.
```
[I] ----------------------------------------------------------------
[I] Input filename:   mnist_cnn.onnx
[I] ONNX IR version:  0.0.7
[I] Opset version:    14
[I] Producer name:    pytorch
[I] Producer version: 1.13.0
[I] Domain:           
[I] Model version:    0
[I] Doc string:       
[I] ----------------------------------------------------------------
```

여기서 생성된 ONNX 파일에 대한 간략한 정보와 이 파일이 어떤 프레임워크에서 생성되었는 지에 대한 정보를 알 수 있다. ONNX에 대한 내용이므로 여기서 자세히 다루지는 않는다.

<br>

# Performing Inference

최적화하고 생성된 플랜을 런타임에서 읽어서 추론하는 과정 또한 [MNIST Classification using Network Definition APIs](/tensorrt/study/01_mnist_cnn_api.md)와 동일하므로 자세히 다루지 않는다.

다만, 한 가지 주의할 점이 하나 있다. TensorRT에서는 기본적으로 텐서의 차원이 2D 이더라도, 나머지 차원을 1로 설정하여 4D로 사용하는 것을 권장한다. 그래서 Network Definition API를 사용할 때, 마지막 Fully Connected 레이어의 입력과 출력이 4D 텐서였다. 하지만, ONNX 파일을 통해 최적화를 수행하니, 출력 텐서의 차원이 2D로 지정되어 있었고, 4D라고 가정하면 마지막 두 차원의 값이 0이므로 출력 텐서의 요소 갯수를 카운트할 때 에러가 발생할 수 있다. 따라서, [mnist_cnn_onnx.cpp](/tensorrt/code/mnist_cnn_onnx/mnist_cnn_onnx.cpp)에서는 엔진에 바인딩되는 텐서의 메모리를 할당할 때, 아래와 같은 방법을 사용했다.
```c++
size_t count = std::accumulate(input_dims.d, input_dims.d + input_dims.nbDims, 1, std::multiplies<>());
input = (void*)malloc(count * sizeof(float));
cudaMalloc(&d_input, count * sizeof(float));

count = std::accumulate(output_dims.d, output_dims.d + output_dims.nbDims, 1, std::multiplies<>());
output = (void*)malloc(count * sizeof(float));
cudaMalloc(&d_output, count * sizeof(float));
```

샘플 데이터에 대한 출력 결과는 다음과 같다.
```
...
............................
............................
............................
............................
........**..................
......******................
.....********...............
......***..****.............
.....***....****............
.....**......****...........
.....**.......***...........
.....**...*...***...........
......*..*********..........
.........***********........
..........************......
..................*****.....
....................****....
.........**..........***....
.........***..........***...
..........**..........***...
..........****......*****...
...........*************....
.............*********......
................*...........
............................
............................
............................
............................
Digit: 3 (0.922231)
Elapsed Time: 0.03584 ms

............................
............................
............................
............................
............................
...........*................
...........*.......**.......
..........**........*.......
..........**........*.......
.........**........**.......
........**.........**.......
........**........***.......
.......**.........**........
.......**........***........
.......**........***........
.......**........**.........
.......***....*****.........
........***********.........
................***.........
.................**.........
................***.........
.................**.........
................***.........
................**..........
................*...........
............................
............................
............................
Digit: 4 (1)
Elapsed Time: 0.033792 ms
...
```

Network Definition API를 사용할 때와 비슷한 시간(약 0,03 ms)이 걸리는 것으로 측정된다.

> Network Definition API를 사용하여 네트워크를 직접 구성하여 최적화했을 때의 속도와 ONNX 파일로 파싱하여 최적화했을 때의 속도에 유의미한 차이는 거의 없는 것 같다.

<br>

# References

- [NVIDIA cuDNN Documentation: The C++ API](/tensorrt/doc/01_developer_guide/03_the_cpp_api.md)
# Table of Contents

- [ldmatrix](#ldmatrix)
- [Instruction Details](#instruction-details)
- [Examples](#examples)
- [References](#references)

# ldmatrix

`ldmatrx`는 `mma` 명령어을 사용하기 위해 warp-level에서 행렬 데이터를 로드하는 역할을 한다. `mma` 명령어를 사용하기 위한 것이라는 말에서 알 수 있듯이 Tensor Core 연산과의 연계를 염두에 둔 명령어이며, Tensor Core에서 활용할 수 있는 최적화된 형식으로 데이터를 로드한다.

# Instruction Details

`ldmatrix`의 기본적인 명령어 구성은 다음과 같다.

```
ldmatrix.sync.aligned.shape.num{.trans}{.ss}.type r, [p];

.shape  = {.m8n8};
.num    = {.x1, .x2, .x4};
.ss     = {.shared{::cta}};
.type   = {.b16};
```

- `sync`: 한 워프내 모든 스레드들이 같은 `ldmatrix` 명령어 실행을 마칠 때까지 각 실행 중인 스레드들을 대기하도록 하여 동기화가 되도록 보장한다.
- `aligned`: 한 워프내 모든 스레드들이 반드시 같은 `ldmatrix` 명령어를 실행해야 한다는 것을 의미한다.
- `shape`: 로드할 행렬의 크기이다. 8x8 행렬 크기를 지원한다.
- `num`: `x1`, `x2`, `x4`라는 값들을 사용할 수 있으며, 각각 행렬의 갯수를 의미한다.
- `trans`: 행렬을 로드할 때, column-major format으로 로드한다 (optional).
- `type`: 명령어를 통해 로드되는 행렬 요소는 16-bit data만 가능하다.
- `ss`: state space를 의미하며, 생략이 가능한 것처럼 표시되어 있는데 생략하면 정상적으로 동작하지 않는 것 같다.

`ldmatrix`는 한 워프 내 모든 스레드들을 통해 `p`가 가리키는 `.shared` state space 위치로부터 레지스터 `r`로 행렬을 로드한다. `p`가 가리키는 주소는 generic addressing이며 `__cvta_generic_to_shared` intrinsic으로부터 shared memory의 generic address를 얻을 수 있다. 자세한 내용은 예제 코드를 통해서 살펴볼 수 있다.

행렬의 각 행은 메모리에 연속적일 필요는 없으며, 행렬의 각 행에 해당하는 주소는 `.num`에 따라서 해당하는 스레드에 제공되면 된다. `.x1`이라면 8개의 행의 주소를 스레드 0-7에 전달하면 된다. `.x2`인 경우에는 추가되는 8개 행의 주소를 스레드 8-15에 전달하면 된다.

|`.num`|Threads 0-7|Threads 8-15|Threads 16-23|Threads24-31|
|--|--|--|--|--|
|`.x1`|addr0 - addr7| - | - | - |
|`.x2`|addr0 - addr7|addr8 - addr15| - | - |
|`.x4`|addr0 - addr7|addr8 - addr15|addr16 - addr23|addr24 - addr31|

8x8 행렬을 읽을 때, 4개의 연속된 스레드가 하나의 행을 로드한다. 즉, 8개의 16-bits 요소, 총 16바이트를 로드하게 된다.

워프 내 각 스레드는 한 행의 일부를 로드하며, 각 스레드의 레지스터에는 아래에 해당하는 요소 값들이 로드된다 (`.x1`인 경우).

<img src="https://docs.nvidia.com/cuda/parallel-thread-execution/_images/mma-ldmatrix-fragments.png" width=500px style="display: block; margin: 0 auto; background-color: white"/>

`.num` = `.x2`라면, 두 번째 행렬의 요소들이 위의 레이아웃대로 각 스레드의 레지스터에 로드된다. 동일하게 `.num`=`.x4`인 경우, 세 번째 및 네 번째 행렬도 동일하게 확장된다.

# Examples

## Load 8x8 matrix

간단하게 device memory에 저장된 8x8 행렬을 shared memory에 로드하고 `ldmatrix` 명령어를 사용하여 각 스레드의 레지스터로 로드한 뒤, 레지스터의 값들을 다시 global memory로 쓰는 커널을 구현하였다.

전체 코드는 [link](/cuda/code/ptr_ldmatrix/test_ldmatrix_x1.cu)에서 확인할 수 있다.

```c++
__global__
void ldmatrix_test_kernel(half* dst, half const* src, int const count)
{
    __shared__ half shmem[64];

    int tid = threadIdx.x;
    int warp_id = tid / 32;

    if (2 * tid < count) {
        shmem[2 * tid] = src[2 * tid];
        shmem[2 * tid + 1] = src[2 * tid + 1];
    }
    __syncthreads();

    half* shmem_ptr = nullptr;
    half frag[2];
    if (tid < 8) {
        shmem_ptr = shmem + 8 * tid;
    }

    uint32_t* reg_ptr = reinterpret_cast<uint32_t*>(frag);
    uint32_t shmem_ptr_val = __cvta_generic_to_shared(shmem_ptr);

    asm volatile(
        "ldmatrix.sync.aligned.m8n8.x1.shared.b16 {%0}, [%1];"
        : "=r"(reg_ptr[0])
        : "r"(shmem_ptr_val)
    );

    // printf("[%2d] %f %f\n", tid, __half2float(frag[0]), __half2float(frag[1]));

    if (2 * tid < count) {
        dst[2 * tid] = frag[0];
        dst[2 * tid + 1] = frag[1];
    }
}
```

위에서 살펴본 레이아웃 그대로 shared memory로부터 각 스레드 레지스터에 값을 로드하고, 이를 다시 그대로 global memory에 썼기 때문에 입력으로 받은 행렬과 동일한 위치에 각 요소들이 다시 쓰여지게 된다.

출력 결과는 다음과 같다.
```
Source Matrix:
  0.0   1.0   2.0   3.0   4.0   5.0   6.0   7.0 
  8.0   9.0  10.0  11.0  12.0  13.0  14.0  15.0 
 16.0  17.0  18.0  19.0  20.0  21.0  22.0  23.0 
 24.0  25.0  26.0  27.0  28.0  29.0  30.0  31.0 
 32.0  33.0  34.0  35.0  36.0  37.0  38.0  39.0 
 40.0  41.0  42.0  43.0  44.0  45.0  46.0  47.0 
 48.0  49.0  50.0  51.0  52.0  53.0  54.0  55.0 
 56.0  57.0  58.0  59.0  60.0  61.0  62.0  63.0 

Result Matrix:
  0.0   1.0   2.0   3.0   4.0   5.0   6.0   7.0 
  8.0   9.0  10.0  11.0  12.0  13.0  14.0  15.0 
 16.0  17.0  18.0  19.0  20.0  21.0  22.0  23.0 
 24.0  25.0  26.0  27.0  28.0  29.0  30.0  31.0 
 32.0  33.0  34.0  35.0  36.0  37.0  38.0  39.0 
 40.0  41.0  42.0  43.0  44.0  45.0  46.0  47.0 
 48.0  49.0  50.0  51.0  52.0  53.0  54.0  55.0 
 56.0  57.0  58.0  59.0  60.0  61.0  62.0  63.0
```

여기서 만약 `.trans`를 명령어에 추가해주면 아래의 결과를 얻을 수 있다.
```
Source Matrix:
  0.0   1.0   2.0   3.0   4.0   5.0   6.0   7.0 
  8.0   9.0  10.0  11.0  12.0  13.0  14.0  15.0 
 16.0  17.0  18.0  19.0  20.0  21.0  22.0  23.0 
 24.0  25.0  26.0  27.0  28.0  29.0  30.0  31.0 
 32.0  33.0  34.0  35.0  36.0  37.0  38.0  39.0 
 40.0  41.0  42.0  43.0  44.0  45.0  46.0  47.0 
 48.0  49.0  50.0  51.0  52.0  53.0  54.0  55.0 
 56.0  57.0  58.0  59.0  60.0  61.0  62.0  63.0 

Result Matrix:
  0.0   8.0  16.0  24.0  32.0  40.0  48.0  56.0 
  1.0   9.0  17.0  25.0  33.0  41.0  49.0  57.0 
  2.0  10.0  18.0  26.0  34.0  42.0  50.0  58.0 
  3.0  11.0  19.0  27.0  35.0  43.0  51.0  59.0 
  4.0  12.0  20.0  28.0  36.0  44.0  52.0  60.0 
  5.0  13.0  21.0  29.0  37.0  45.0  53.0  61.0 
  6.0  14.0  22.0  30.0  38.0  46.0  54.0  62.0 
  7.0  15.0  23.0  31.0  39.0  47.0  55.0  63.0
```

## Load 8x16 matrix

이번에는 `.num`=`.x2`를 이용하여 8x16 행렬을 로드하는 커널이다. 왼쪽 8x8 행렬과 오른쪽 8x8이 `ldmatrix` 하나의 명령어로 로드되는 것으로 볼 수 있다. 각 스레드가 4개의 요소를 로드한 다음, global memory로 4개의 레지스터에 저장된 값을 연속된 위치에 쓴다.

전체 코드는 [link](/cuda/code/ptr_ldmatrix/test_ldmatrix_x2.cu)에서 확인할 수 있다.

```c++
__global__
void ldmatrix_test_kernel(half* dst, half const* src, int const count)
{
    __shared__ half shmem[128];

    int tid = threadIdx.x;

    if (4 * tid < count) {
        shmem[4 * tid] = src[4 * tid];
        shmem[4 * tid + 1] = src[4 * tid + 1];
        shmem[4 * tid + 2] = src[4 * tid + 2];
        shmem[4 * tid + 3] = src[4 * tid + 3];
    }
    __syncthreads();

    half* shmem_ptr = nullptr;
    half frag[4];
    if (tid < 16) {
        shmem_ptr = shmem + 16 * (tid % 8) + (tid / 8) * 8;
    }

    uint32_t* reg_ptr = reinterpret_cast<uint32_t*>(frag);
    uint32_t shmem_ptr_val = __cvta_generic_to_shared(shmem_ptr);

    asm volatile(
        "ldmatrix.sync.aligned.m8n8.x2.shared.b16 {%0, %1}, [%2];"
        : "=r"(reg_ptr[0]), "=r"(reg_ptr[1])
        : "r"(shmem_ptr_val)
    );

    // printf("[%2d] %f %f %f %f\n", tid, __half2float(frag[0]), __half2float(frag[1]), __half2float(frag[2]), __half2float(frag[3]));

    if (4 * tid < count) {
        dst[4 * tid] = frag[0];
        dst[4 * tid + 1] = frag[1];
        dst[4 * tid + 2] = frag[2];
        dst[4 * tid + 3] = frag[3];
    }
}
```

로드된 행렬의 배치는 아래와 같이 바뀌는 것을 확인할 수 있다.

```
Source Matrix:
  0.0   1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0  10.0  11.0  12.0  13.0  14.0  15.0 
 16.0  17.0  18.0  19.0  20.0  21.0  22.0  23.0  24.0  25.0  26.0  27.0  28.0  29.0  30.0  31.0 
 32.0  33.0  34.0  35.0  36.0  37.0  38.0  39.0  40.0  41.0  42.0  43.0  44.0  45.0  46.0  47.0 
 48.0  49.0  50.0  51.0  52.0  53.0  54.0  55.0  56.0  57.0  58.0  59.0  60.0  61.0  62.0  63.0 
 64.0  65.0  66.0  67.0  68.0  69.0  70.0  71.0  72.0  73.0  74.0  75.0  76.0  77.0  78.0  79.0 
 80.0  81.0  82.0  83.0  84.0  85.0  86.0  87.0  88.0  89.0  90.0  91.0  92.0  93.0  94.0  95.0 
 96.0  97.0  98.0  99.0 100.0 101.0 102.0 103.0 104.0 105.0 106.0 107.0 108.0 109.0 110.0 111.0 
112.0 113.0 114.0 115.0 116.0 117.0 118.0 119.0 120.0 121.0 122.0 123.0 124.0 125.0 126.0 127.0 

Result Matrix:
  0.0   1.0   8.0   9.0   2.0   3.0  10.0  11.0   4.0   5.0  12.0  13.0   6.0   7.0  14.0  15.0 
 16.0  17.0  24.0  25.0  18.0  19.0  26.0  27.0  20.0  21.0  28.0  29.0  22.0  23.0  30.0  31.0 
 32.0  33.0  40.0  41.0  34.0  35.0  42.0  43.0  36.0  37.0  44.0  45.0  38.0  39.0  46.0  47.0 
 48.0  49.0  56.0  57.0  50.0  51.0  58.0  59.0  52.0  53.0  60.0  61.0  54.0  55.0  62.0  63.0 
 64.0  65.0  72.0  73.0  66.0  67.0  74.0  75.0  68.0  69.0  76.0  77.0  70.0  71.0  78.0  79.0 
 80.0  81.0  88.0  89.0  82.0  83.0  90.0  91.0  84.0  85.0  92.0  93.0  86.0  87.0  94.0  95.0 
 96.0  97.0 104.0 105.0  98.0  99.0 106.0 107.0 100.0 101.0 108.0 109.0 102.0 103.0 110.0 111.0 
112.0 113.0 120.0 121.0 114.0 115.0 122.0 123.0 116.0 117.0 124.0 125.0 118.0 119.0 126.0 127.0
```

Thread 0에서는 첫 번째 행렬의 1행 1-2열의 요소(0, 1)와 두 번째 행렬의 1행 1-2열의 요소(8, 9)를 로드하고, 이 값들을 결과 행렬의 연속된 위치에 쓰기 때문에 위와 같이 배치가 바뀐 것을 확인할 수 있다.

`.trans`를 사용하면 아래와 같이 결과가 바뀐다.
```
Source Matrix:
  0.0   1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0  10.0  11.0  12.0  13.0  14.0  15.0 
 16.0  17.0  18.0  19.0  20.0  21.0  22.0  23.0  24.0  25.0  26.0  27.0  28.0  29.0  30.0  31.0 
 32.0  33.0  34.0  35.0  36.0  37.0  38.0  39.0  40.0  41.0  42.0  43.0  44.0  45.0  46.0  47.0 
 48.0  49.0  50.0  51.0  52.0  53.0  54.0  55.0  56.0  57.0  58.0  59.0  60.0  61.0  62.0  63.0 
 64.0  65.0  66.0  67.0  68.0  69.0  70.0  71.0  72.0  73.0  74.0  75.0  76.0  77.0  78.0  79.0 
 80.0  81.0  82.0  83.0  84.0  85.0  86.0  87.0  88.0  89.0  90.0  91.0  92.0  93.0  94.0  95.0 
 96.0  97.0  98.0  99.0 100.0 101.0 102.0 103.0 104.0 105.0 106.0 107.0 108.0 109.0 110.0 111.0 
112.0 113.0 114.0 115.0 116.0 117.0 118.0 119.0 120.0 121.0 122.0 123.0 124.0 125.0 126.0 127.0 

Result Matrix:
  0.0  16.0   8.0  24.0  32.0  48.0  40.0  56.0  64.0  80.0  72.0  88.0  96.0 112.0 104.0 120.0 
  1.0  17.0   9.0  25.0  33.0  49.0  41.0  57.0  65.0  81.0  73.0  89.0  97.0 113.0 105.0 121.0 
  2.0  18.0  10.0  26.0  34.0  50.0  42.0  58.0  66.0  82.0  74.0  90.0  98.0 114.0 106.0 122.0 
  3.0  19.0  11.0  27.0  35.0  51.0  43.0  59.0  67.0  83.0  75.0  91.0  99.0 115.0 107.0 123.0 
  4.0  20.0  12.0  28.0  36.0  52.0  44.0  60.0  68.0  84.0  76.0  92.0 100.0 116.0 108.0 124.0 
  5.0  21.0  13.0  29.0  37.0  53.0  45.0  61.0  69.0  85.0  77.0  93.0 101.0 117.0 109.0 125.0 
  6.0  22.0  14.0  30.0  38.0  54.0  46.0  62.0  70.0  86.0  78.0  94.0 102.0 118.0 110.0 126.0 
  7.0  23.0  15.0  31.0  39.0  55.0  47.0  63.0  71.0  87.0  79.0  95.0 103.0 119.0 111.0 127.0
```

## Load 16x16 matrix

이번에는 16x16 행렬을 `ldmatrix`를 통해 레지스터로 로드하고 다시 global memory로 쓰는 커널이다. 전체 코드는 [link](/cuda/code/ptr_ldmatrix/test_ldmatrix_x4.cu)에서 확인할 수 있다.

```c++
__global__
void ldmatrix_test_kernel(half* dst, half const* src, int const count)
{
    __shared__ half shmem[256];

    int tid = threadIdx.x;
    int group_idx = tid / 8;

    if (8 * tid < count) {
        shmem[8 * tid] = src[8 * tid];
        shmem[8 * tid + 1] = src[8 * tid + 1];
        shmem[8 * tid + 2] = src[8 * tid + 2];
        shmem[8 * tid + 3] = src[8 * tid + 3];
        shmem[8 * tid + 4] = src[8 * tid + 4];
        shmem[8 * tid + 5] = src[8 * tid + 5];
        shmem[8 * tid + 6] = src[8 * tid + 6];
        shmem[8 * tid + 7] = src[8 * tid + 7];
    }
    __syncthreads();

    half* shmem_ptr = nullptr;
    half frag[8];
    if (tid < 32) {
        shmem_ptr = shmem + 16 * ((group_idx / 2) * 8 + tid % 8) + (group_idx % 2) * 8;
    }

    uint32_t* reg_ptr = reinterpret_cast<uint32_t*>(frag);
    uint32_t shmem_ptr_val = __cvta_generic_to_shared(shmem_ptr);

    asm volatile(
        "ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%0, %1, %2, %3}, [%4];"
        : "=r"(reg_ptr[0]), "=r"(reg_ptr[1]), "=r"(reg_ptr[2]), "=r"(reg_ptr[3])
        : "r"(shmem_ptr_val)
    );

    // printf("[%2d] %f %f %f %f %f %f %f %f\n", tid,
    //     __half2float(frag[0]), __half2float(frag[1]), __half2float(frag[2]), __half2float(frag[3]),
    //     __half2float(frag[4]), __half2float(frag[5]), __half2float(frag[6]), __half2float(frag[7]));

    if (8 * tid < count) {
        dst[8 * tid] = frag[0];
        dst[8 * tid + 1] = frag[1];
        dst[8 * tid + 2] = frag[2];
        dst[8 * tid + 3] = frag[3];
        dst[8 * tid + 4] = frag[4];
        dst[8 * tid + 5] = frag[5];
        dst[8 * tid + 6] = frag[6];
        dst[8 * tid + 7] = frag[7];
    }
}
```

이번에는 16x16 행렬을 왼쪽 위, 오른쪽 위, 왼쪽 아래, 오른쪽 아래 순으로 8x8 행렬 4개를 그룹화하여 각 행의 주소를 대응하는 스레드에서 `ldmatrix` 명령어로 넘겨주게 된다. 

Thread 0은 각 8x8 행렬의 1행 1,2열의 요소이므로 0, 1, 8, 9, 128, 129, 136, 137의 값을 로드하게 된다. 따라서, 아래와 같은 결과를 얻을 수 있다.

```
Source Matrix:
  0.0   1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0  10.0  11.0  12.0  13.0  14.0  15.0 
 16.0  17.0  18.0  19.0  20.0  21.0  22.0  23.0  24.0  25.0  26.0  27.0  28.0  29.0  30.0  31.0 
 32.0  33.0  34.0  35.0  36.0  37.0  38.0  39.0  40.0  41.0  42.0  43.0  44.0  45.0  46.0  47.0 
 48.0  49.0  50.0  51.0  52.0  53.0  54.0  55.0  56.0  57.0  58.0  59.0  60.0  61.0  62.0  63.0 
 64.0  65.0  66.0  67.0  68.0  69.0  70.0  71.0  72.0  73.0  74.0  75.0  76.0  77.0  78.0  79.0 
 80.0  81.0  82.0  83.0  84.0  85.0  86.0  87.0  88.0  89.0  90.0  91.0  92.0  93.0  94.0  95.0 
 96.0  97.0  98.0  99.0 100.0 101.0 102.0 103.0 104.0 105.0 106.0 107.0 108.0 109.0 110.0 111.0 
112.0 113.0 114.0 115.0 116.0 117.0 118.0 119.0 120.0 121.0 122.0 123.0 124.0 125.0 126.0 127.0 
128.0 129.0 130.0 131.0 132.0 133.0 134.0 135.0 136.0 137.0 138.0 139.0 140.0 141.0 142.0 143.0 
144.0 145.0 146.0 147.0 148.0 149.0 150.0 151.0 152.0 153.0 154.0 155.0 156.0 157.0 158.0 159.0 
160.0 161.0 162.0 163.0 164.0 165.0 166.0 167.0 168.0 169.0 170.0 171.0 172.0 173.0 174.0 175.0 
176.0 177.0 178.0 179.0 180.0 181.0 182.0 183.0 184.0 185.0 186.0 187.0 188.0 189.0 190.0 191.0 
192.0 193.0 194.0 195.0 196.0 197.0 198.0 199.0 200.0 201.0 202.0 203.0 204.0 205.0 206.0 207.0 
208.0 209.0 210.0 211.0 212.0 213.0 214.0 215.0 216.0 217.0 218.0 219.0 220.0 221.0 222.0 223.0 
224.0 225.0 226.0 227.0 228.0 229.0 230.0 231.0 232.0 233.0 234.0 235.0 236.0 237.0 238.0 239.0 
240.0 241.0 242.0 243.0 244.0 245.0 246.0 247.0 248.0 249.0 250.0 251.0 252.0 253.0 254.0 255.0 

Result Matrix:
  0.0   1.0   8.0   9.0 128.0 129.0 136.0 137.0   2.0   3.0  10.0  11.0 130.0 131.0 138.0 139.0 
  4.0   5.0  12.0  13.0 132.0 133.0 140.0 141.0   6.0   7.0  14.0  15.0 134.0 135.0 142.0 143.0 
 16.0  17.0  24.0  25.0 144.0 145.0 152.0 153.0  18.0  19.0  26.0  27.0 146.0 147.0 154.0 155.0 
 20.0  21.0  28.0  29.0 148.0 149.0 156.0 157.0  22.0  23.0  30.0  31.0 150.0 151.0 158.0 159.0 
 32.0  33.0  40.0  41.0 160.0 161.0 168.0 169.0  34.0  35.0  42.0  43.0 162.0 163.0 170.0 171.0 
 36.0  37.0  44.0  45.0 164.0 165.0 172.0 173.0  38.0  39.0  46.0  47.0 166.0 167.0 174.0 175.0 
 48.0  49.0  56.0  57.0 176.0 177.0 184.0 185.0  50.0  51.0  58.0  59.0 178.0 179.0 186.0 187.0 
 52.0  53.0  60.0  61.0 180.0 181.0 188.0 189.0  54.0  55.0  62.0  63.0 182.0 183.0 190.0 191.0 
 64.0  65.0  72.0  73.0 192.0 193.0 200.0 201.0  66.0  67.0  74.0  75.0 194.0 195.0 202.0 203.0 
 68.0  69.0  76.0  77.0 196.0 197.0 204.0 205.0  70.0  71.0  78.0  79.0 198.0 199.0 206.0 207.0 
 80.0  81.0  88.0  89.0 208.0 209.0 216.0 217.0  82.0  83.0  90.0  91.0 210.0 211.0 218.0 219.0 
 84.0  85.0  92.0  93.0 212.0 213.0 220.0 221.0  86.0  87.0  94.0  95.0 214.0 215.0 222.0 223.0 
 96.0  97.0 104.0 105.0 224.0 225.0 232.0 233.0  98.0  99.0 106.0 107.0 226.0 227.0 234.0 235.0 
100.0 101.0 108.0 109.0 228.0 229.0 236.0 237.0 102.0 103.0 110.0 111.0 230.0 231.0 238.0 239.0 
112.0 113.0 120.0 121.0 240.0 241.0 248.0 249.0 114.0 115.0 122.0 123.0 242.0 243.0 250.0 251.0 
116.0 117.0 124.0 125.0 244.0 245.0 252.0 253.0 118.0 119.0 126.0 127.0 246.0 247.0 254.0 255.0
```

# References

- [Warp-level matrix load instruction: ldmatrix](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-load-instruction-ldmatrix)